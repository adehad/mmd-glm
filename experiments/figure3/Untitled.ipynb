{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.functional import conv1d\n",
    "\n",
    "# k, e_dc, u, r_train_dc_ml = glm.sample_conditioned(st_train.t, st_train.mask, full=True)\n",
    "# k, e_fr, _, r_fr_ml_long, mask_spikes_fr_ml_long = glm.sample(t_long, shape=(20,), full=True)\n",
    "\n",
    "# model = glm\n",
    "\n",
    "# r1 = torch.from_numpy(r_train_dc_ml)\n",
    "# r2 = torch.from_numpy(r_fr_ml_long)\n",
    "\n",
    "# eta1 = torch.log(r1) - model.b\n",
    "# eta2 = torch.log(r2) - model.b\n",
    "\n",
    "# # autocor1 = np.mean(auto_covariance(e_dc, subtract_mean=False, stationary_signal=False, biased=True), 1)\n",
    "# # autocor2 = np.mean(auto_covariance(e_fr, subtract_mean=False, stationary_signal=False, biased=True), 1)\n",
    "# autocor1 = np.mean(auto_covariance(eta1.detach().numpy(), subtract_mean=False, negative_times=False, \n",
    "#                                    stationary_signal=False, biased=True), 1)\n",
    "# autocor2 = np.mean(auto_covariance(eta2.detach().numpy(), subtract_mean=False, negative_times=False, \n",
    "#                                    stationary_signal=False, biased=True), 1)\n",
    "\n",
    "# T1, T2 = eta1.shape[0], eta2.shape[0]\n",
    "\n",
    "# autocor1pt = conv1d(eta1.T[None, :, :], eta1.T[:, None, :], padding=(T1 - 1), groups=eta1.shape[1]) / T1\n",
    "# autocor1pt = autocor1pt[0, :, (T1-1):].T\n",
    "\n",
    "# autocor2pt = conv1d(eta2.T[None, :, :], eta2.T[:, None, :], padding=(T2 - 1), groups=eta2.shape[1]) / T2\n",
    "# autocor2pt = autocor2pt[0, :, (T2-1):].T\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "# ax1.plot(autocor1)\n",
    "# ax1.plot(autocor2)\n",
    "# ax1.plot(torch.mean(autocor1pt, 1).detach(), '--')\n",
    "# ax1.plot(torch.mean(autocor2pt, 1).detach(), '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.functional import conv1d\n",
    "\n",
    "# k, e_dc, u, r_train_dc_ml = glm.sample_conditioned(st_train.t, st_train.mask, full=True)\n",
    "# # k, e_fr, _, r_fr_ml_long, mask_spikes_fr_ml_long = glm.sample(t_long, shape=(20,), full=True)\n",
    "\n",
    "# model = glm\n",
    "\n",
    "# r1 = torch.from_numpy(r_train_dc_ml)\n",
    "# eta1 = torch.log(r1) - model.b\n",
    "\n",
    "# T = eta1.shape[0]\n",
    "\n",
    "# autocor1 = auto_covariance(eta1.detach().numpy(), subtract_mean=False, negative_times=False, \n",
    "#                                    stationary_signal=False, biased=True)\n",
    "\n",
    "# # eta1 = eta1.T[:, None, :]\n",
    "# # eta2 = eta2.T[:, None, :]\n",
    "\n",
    "# # eta1 = eta1.T[None, :, :]\n",
    "# autocor1pt = conv1d(eta1.T[None, :, :], eta1.T[:, None, :], padding=(eta1.shape[0] - 1), groups=10) / T\n",
    "# autocor1pt = autocor1pt[:, :, (T-1):]\n",
    "# # aux = torch.mean(autocor1pt.detach()[0], dim=0)[autocor1pt.shape[2] // 2:]\n",
    "\n",
    "# # autocor1pt = conv1d(eta1, eta1, padding=(eta1.shape[2] - 1) // 2)\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "# ax1.plot(autocor1[:, 0])\n",
    "# ax2.plot(autocor1[:, 1])\n",
    "# ax1.plot(autocor1pt.detach()[0, 0, :])\n",
    "# ax2.plot(autocor1pt.detach()[0, 1, :])\n",
    "\n",
    "# # ax1.plot(aux)\n",
    "# # ax1.plot(torch.mean(eta1.detach(), 0)[0, :])\n",
    "\n",
    "# # conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
